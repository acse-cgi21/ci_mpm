{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKi5M10klMCN"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1dtR470-kr9MhA_vytTgnLtdrr5X7HaXu\" width=\"150\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zc-rCXSX7KDX"
      },
      "source": [
        "### Name: Ijeh, Chukwume\n",
        "### CID: 02140724"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VALo7r5bg2_S"
      },
      "source": [
        "# Instructions:\n",
        "\n",
        "Follow the instructions below to complete the coursework and submit it:\n",
        "\n",
        "1. Read the materials you will need to complete the coursework; you can find them in the github classroom repo provided together with this notebook. The provided materials are: \n",
        "\n",
        "  - The paper [*Gradient based learning applied to document recognition*](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf) by Yann LeCunn where he describes the original form of the LeNet-5 architecture.\n",
        "\n",
        "  - The paper [*Visualizing the Loss Landscape of Neural Nets*](https://arxiv.org/pdf/1712.09913.pdf).\n",
        "\n",
        "  - This Jupyter Notebook template to fill in your answers (it contains some code to assist you).\n",
        "\n",
        "\n",
        "2. Complete your coursework using this provided Jupyter Notebook template (use Google Colab or your local machine if it has a GPU and/or sufficient computational power).\n",
        "\n",
        "3. Once you have completed your answers, upload your final notebook to the repo you got from the github classroom link (as you normally do). Make sure to have all the answers in there:\n",
        "\n",
        "   - **All the cells in your final Jupyter Notebook should be executed before saving and uploading to github in order to have the output of the cells available in the uploaded version** (images you plot, training graphs generated with `livelossplot`, etc). We will not guarantee that if some code blocks are missing we will be able to rerun them for you.\n",
        "\n",
        "   - Add comments in the code to explain what you are doing at every step. \n",
        "\n",
        "   - All answers requiring written answers (ie, not code) should be in markdown blocks in the Jupyter Notebook. This provided Jupyter Notebook template has allocated blocks for the questions, but you can add any coding or markdown blocks that you need.\n",
        "\n",
        "4. The coursework is released on **Friday 13 May at 15:30h BST**, and the answers have to be submitted on **Saturday  21 May, 23:59h BST**. We will not accept late submissions.\n",
        "\n",
        "    ***[BST stands for British Summer Time (local UK time)]***\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "<br>\n",
        "\n",
        "### The coursework consists of a number of questions/exercises you have to complete. You will find them below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uwm3nR2K5u98"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pycm livelossplot\n",
        "%pylab inline"
      ],
      "metadata": {
        "id": "ARbtdPSvJj4g",
        "outputId": "f51b3e61-a670-4813-cf5c-bc53ad4d84eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pycm in /usr/local/lib/python3.7/dist-packages (3.5)\n",
            "Requirement already satisfied: livelossplot in /usr/local/lib/python3.7/dist-packages (0.5.5)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from pycm) (1.21.6)\n",
            "Requirement already satisfied: art>=1.8 in /usr/local/lib/python3.7/dist-packages (from pycm) (5.6)\n",
            "Requirement already satisfied: ipython==7.* in /usr/local/lib/python3.7/dist-packages (from livelossplot) (7.33.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from livelossplot) (3.2.2)\n",
            "Requirement already satisfied: bokeh in /usr/local/lib/python3.7/dist-packages (from livelossplot) (2.3.3)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.7/dist-packages (from ipython==7.*->livelossplot) (0.18.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython==7.*->livelossplot) (0.7.5)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython==7.*->livelossplot) (57.4.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython==7.*->livelossplot) (4.4.2)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.7/dist-packages (from ipython==7.*->livelossplot) (0.1.3)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython==7.*->livelossplot) (3.0.29)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.7/dist-packages (from ipython==7.*->livelossplot) (4.8.0)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython==7.*->livelossplot) (5.1.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython==7.*->livelossplot) (0.2.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython==7.*->livelossplot) (2.6.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.16->ipython==7.*->livelossplot) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect>4.3->ipython==7.*->livelossplot) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython==7.*->livelossplot) (0.2.5)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot) (3.13)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot) (4.2.0)\n",
            "Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot) (7.1.2)\n",
            "Requirement already satisfied: Jinja2>=2.9 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot) (2.11.3)\n",
            "Requirement already satisfied: packaging>=16.8 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot) (21.3)\n",
            "Requirement already satisfied: tornado>=5.1 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot) (5.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.9->bokeh->livelossplot) (2.0.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=16.8->bokeh->livelossplot) (3.0.8)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->bokeh->livelossplot) (1.15.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->livelossplot) (1.4.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->livelossplot) (0.11.0)\n",
            "Populating the interactive namespace from numpy and matplotlib\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2OeIX5M25t7K"
      },
      "outputs": [],
      "source": [
        "# your imports and initial checks here\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "from livelossplot import PlotLosses\n",
        "from pycm import *\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from torchsummary import summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBoy1vCb68TB"
      },
      "source": [
        "## 1-Load the medical MNIST dataset [10 points]\n",
        "Go to this [kaggle dataset](https://www.kaggle.com/datasets/andrewmvd/medical-mnist/code) and download the data (see provided snippets below).\n",
        "\n",
        "Format the data as you see fit to use it in your next questions. Explain the process you follow.\n",
        "\n",
        "[**It is recommended that you convert the images to grayscale using** *torchvision transforms*]\n",
        "\n",
        "Use the following snippets of code to help you download the data first:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "id": "mFxcY6s-Llgv",
        "outputId": "bcd9c0c7-b0e7-4e89-8314-29e5da1b669f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-RYabwg65rdL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a73e2ddc-353b-47ea-e7db-a2a942e36c01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n",
            "cp: cannot stat '/content/drive/MyDrive/kaggle.json': No such file or directory\n",
            "chmod: cannot access '/root/.kaggle/kaggle.json': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "# Get kaggle authentication json\n",
        "! mkdir ~/.kaggle\n",
        "! cp /content/drive/MyDrive/kaggle.json ~/.kaggle/\n",
        "! ls ~/.kaggle\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "T3p8nPPIgYXt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d49a495c-6d37-4b73-f9bf-f3575fbf12e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/kaggle\", line 5, in <module>\n",
            "    from kaggle.cli import main\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/kaggle/__init__.py\", line 23, in <module>\n",
            "    api.authenticate()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/kaggle/api/kaggle_api_extended.py\", line 166, in authenticate\n",
            "    self.config_file, self.config_dir))\n",
            "OSError: Could not find kaggle.json. Make sure it's located in /root/.kaggle. Or use the environment method.\n",
            "mkdir: cannot create directory ‘./medical-mnist’: File exists\n",
            "unzip:  cannot find or open ./medical-mnist.zip, ./medical-mnist.zip.zip or ./medical-mnist.zip.ZIP.\n"
          ]
        }
      ],
      "source": [
        "# Download and unzip dataset\n",
        "! kaggle datasets download -d andrewmvd/medical-mnist\n",
        "! mkdir ./medical-mnist\n",
        "! unzip -q ./medical-mnist.zip -d medical-mnist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YMisK8y8CDU"
      },
      "source": [
        "## 2-Plot data [5 points]\n",
        "Plot 25 images of the training set together with their corresponding label names.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QfJY68Sx8-nn"
      },
      "outputs": [],
      "source": [
        "# your code goes here\n",
        "def show_batch(dataset, nr=4, nc=4):\n",
        "  fig, axarr = plt.subplots(nr, nc, figsize=(10, 10))\n",
        "  for i in range(nr):\n",
        "      for j in range(nc):\n",
        "          idx = random.randint(0, len(train_ds))\n",
        "          sample, target = train_ds[idx]\n",
        "          try:\n",
        "            axarr[i][j].imshow(sample) # if PIL\n",
        "          except:\n",
        "            axarr[i][j].imshow(sample.permute(1,2,0)) # if tensor of shape CHW\n",
        "          target_name = train_ds.classes[target]\n",
        "          axarr[i][j].set_title(\"%s (%i)\"%(target_name, target))\n",
        "\n",
        "  fig.tight_layout(pad=1.5)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "show_batch(train_ds, 5, 5)"
      ],
      "metadata": {
        "id": "Oh0MC_7tbFNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cl5euEpd_sUj"
      },
      "source": [
        "## 3-Prepare your LeNet-5 network [10 points]\n",
        "Use the code provided in the Jupyter Notebook template and modify it as you see fit to be able to perform a forward pass using a single dummy tensor input **x**.\n",
        "\n",
        "Do you need to modify the size of any layers of the network for the Medical MNIST dataset? If yes, explain how you did it and implement the changes.\n",
        "\n",
        "The lines of code that will do the forward pass and print the network are provided in the template."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xzjg5It0_r8e"
      },
      "outputs": [],
      "source": [
        "#     make modifications in the code below\n",
        "\n",
        "class LeNet5(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(LeNet5, self).__init__()\n",
        "    self.c1 = nn.Conv2d(3, 6, kernel_size=5, stride=1, padding=1) \n",
        "    self.s2 = nn.MaxPool2d(kernel_size=2, stride=2)   \n",
        "    self.c3 = nn.Conv2d(6, 16, kernel_size=5, stride=1)   \n",
        "    self.s4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "    self.c5 = nn.Linear(16*5*5, 120)    \n",
        "    self.f6 = nn.Linear(120, 84)   \n",
        "    self.output = nn.Linear(84, 10)  \n",
        "    self.act = nn.ReLU()           \n",
        "    \n",
        "  def forward(self, x):\n",
        "    x = self.act(self.c1(x))    \n",
        "    x = self.act(self.s2(x))   \n",
        "    x = self.act(self.c3(x))   \n",
        "    x = self.act(self.s4(x))    \n",
        "    x = x.view(-1, x.size(1)*x.size(2)*x.size(3))     \n",
        "    x = self.act(self.c5(x))       \n",
        "    x = self.act(self.f6(x))    \n",
        "    return self.output(x)          \n",
        "  \n",
        "x = torch.randn((1, 1, 28, 28)) # Dummy input of the same size as the Medical-MNIST images\n",
        "## generate a dummy tensor x\n",
        "model = LeNet5() ## get an instance of your model and call it model\n",
        "y = model(x) # if you have called your instance something different than model, modify this line of code\n",
        "print(model)\n",
        "print(x.shape, y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDo89tMa9F75"
      },
      "source": [
        "## 4-Create a training, validation split [5 points]\n",
        "Split the data using `sklearn.model_selection.StratifiedShuffleSplit`:\n",
        "\n",
        "- 90\\% of the data in the training set\n",
        "- 10\\% of the data in the validation set\n",
        "\n",
        "Verify the stratified shuffle split by plotting a histogram of the classes in the training and validation set.\n",
        "\n",
        "Prepare the downloaded datasets to be used with your modified network from the previous section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6hqepj8NYw57"
      },
      "outputs": [],
      "source": [
        "# your code goes here\n",
        "shuffler = StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=42).split(mnist_train.train_data, mnist_train.train_labels) \n",
        "indices = [(train_idx, validation_idx) for train_idx, validation_idx in shuffler][0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_standardization(X): # define a standardisation function\n",
        "  X /= 255.       # change these values\n",
        "  X -= 0.1307     # change these values\n",
        "  X /= 0.3081     # change these values\n",
        "  return X"
      ],
      "metadata": {
        "id": "1YQnZfoXhP2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# standardise the data\n",
        "X_train, y_train = apply_standardization(mnist_train.train_data[indices[0]].float()), mnist_train.train_labels[indices[0]]\n",
        "X_val, y_val = apply_standardization(mnist_train.train_data[indices[1]].float()), mnist_train.train_labels[indices[1]]"
      ],
      "metadata": {
        "id": "c1Icrkj_hekG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jsv6RASWAUCC"
      },
      "source": [
        "## 5-Grid search [20 points]\n",
        "From  the  list  below,  select  two  hyperparameters  and  perform  a  2D  grid-search  to  find  the  optimal values for these two hyperparameters.  The range of values to test are provided.  Justify your choice of the two hyperparameters you want to tune (write a paragraph in a markdown cell explaining why you chose these two particular parameters). \n",
        "\n",
        "The list of hyperparameters to choose from is:\n",
        "\n",
        "a)  Random Number Seed:  **42**  [31, 42, 53] \\\n",
        "b)  Learning Rate:  **1e-2**  [1e-1, 1e-2, 1e-3] \\\n",
        "c)  Momentum:  **0.5**  [0.2, 0.5, 0.8] \\\n",
        "d)  Batch Size:  **64**  [64, 128, 512] \\\n",
        "e)  Number of epochs:  **30**  [10, 30, 50]\n",
        "\n",
        "The **values in bold** next to each hyperparameter are the values you need to use if you are not tuning this particular hyperparameter.  The values between square brackets are the values to use if you choose to tune this particular hyperparameter. Fixed hyperparameters:\n",
        "\n",
        "- Optimiser: SGD+momentum\n",
        "- Test batch size: 1000\n",
        "\n",
        "Write the results in two tables (one for the loss and one for the accuracy) where the columns and rows are the first and second hyperparameter have selected. You can use markdown tables or create the table in python.\n",
        "\n",
        "Using *livelossplot* to display the training of some grid-search runs (don't need to plot all of them, choose the ones you think are more relevant to explain your hyperparameter search strategy).\n",
        "\n",
        "Select the best values for the two hyperparameters you have chosen to optimise and **justify your choice**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gl2FDyyyoGon"
      },
      "outputs": [],
      "source": [
        "# your code goes here\n",
        "mnist_train = TensorDataset(X_train, y_train.long())    # PyTorch wants longs for categorical data\n",
        "mnist_validate = TensorDataset(X_val, y_val.long())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(momentum):          # define function train_momentum which accepts one argument (momentum)\n",
        "  set_seed(seed)                    # set seed\n",
        "  model = SimpleNet().to(device)    # instantiate model and send it to the GPU\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)  # define an optimiser\n",
        "  criterion = nn.CrossEntropyLoss() # define the loss function\n",
        "  \n",
        "  train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=0)               # train_loader\n",
        "  validation_loader = DataLoader(mnist_validate, batch_size=test_batch_size, shuffle=False, num_workers=0) # validation_loader\n",
        "  test_loader = DataLoader(mnist_test, batch_size=test_batch_size, shuffle=False, num_workers=0)           # test_loader\n",
        "  \n",
        "  liveloss = PlotLosses()\n",
        "  for epoch in range(30):\n",
        "      logs = {}\n",
        "      train_loss, train_accuracy = train(model, optimizer, criterion, train_loader)\n",
        "\n",
        "      logs['log loss'] = train_loss.item()\n",
        "      logs['accuracy'] = train_accuracy.item()\n",
        "\n",
        "      validation_loss, validation_accuracy = validate(model, criterion, validation_loader)\n",
        "      logs['val_log loss'] = validation_loss.item()\n",
        "      logs['val_accuracy'] = validation_accuracy.item()\n",
        "\n",
        "      liveloss.update(logs)\n",
        "      liveloss.draw()\n",
        "      \n",
        "  return True"
      ],
      "metadata": {
        "id": "2lZYoSexj-Sm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(0.1)"
      ],
      "metadata": {
        "id": "YTKumYGRkAfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qvD9_72B3oV"
      },
      "source": [
        "## 6-Train with best hyperparameters [5 points]\n",
        "\n",
        "Once you have your two best hyperparameters, retrain the model by combining your split datasets **as you see fit** and explain why you chose this particular combination. Report the final accuracy on the test set. Use *livelossplot* to plot the values of the training evolution and explain changes in performance with your new combination of datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CzNWdXTErVp"
      },
      "outputs": [],
      "source": [
        "# your code goes here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ka9eNo7EuVF"
      },
      "source": [
        "## 7-Comparison with original LeNet implementation  [10 points]}\n",
        "Read the paper *Gradient based learning applied to document recognition* provided. What are the main differences between their implementation and the one you are using in this coursework?\n",
        "\n",
        "[**You only need to read and understand the paper until section** III. RESULTS AND COMPARISON WITH OTHER METHODS, *B. Results*, **included, which finishes on page 11 (you can ignore the rest)**]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olOyVoP0mzUN"
      },
      "source": [
        "#### your answers here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZcLQ44cFIBi"
      },
      "source": [
        "## 8-Number of parameters in LeNet-5 [5 points]\n",
        "\n",
        "Calculate how many trainable parameters the LeNet network you have implemented for this coursework has, and compare it to the number of parameters of the original LeNet network described in the paper you read in the previous question. Explain how you calculate the number of parameters for both cases.\n",
        "\n",
        "[**answers that only contain the number of parameters without justification will be awarded 0 points**]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6lAyyugRFqwp"
      },
      "outputs": [],
      "source": [
        "# your code goes here (if you want to use a code cell in this question)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKK1N1R8nVFa"
      },
      "source": [
        "#### your answers here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDk6JOtyGKZe"
      },
      "source": [
        "## 9-Visualising loss landscapes paper - 1 [10 points]\n",
        "\n",
        "Read the provided paper *Visualising the Loss Landscape of Neural Nets*. This paper contains a lot of advanced concepts, but you only need to read and understand it well up to and including section 4 (Proposed Visualisation: Filter-Wise Normalisation) to answer the questions below. In section 4 you don't need to fully understand the rationale for doing Filter-Wise Normalisation, but you do need to understand what Filter-Wise Normalisation is.\n",
        "\n",
        "Answer the following question (in a markdown cell):\n",
        "- What are the dimensions of the parameters $\\delta$, $\\eta$, $\\alpha$ and $\\beta$ in equation (1) using your LeNet network? [5 points]\n",
        "- Describe what is the role of each of these parameters [5 points]\n",
        "\n",
        "Explain in detail and justify your answers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-iB3cUwH86e"
      },
      "source": [
        "## 10-Visualise loss landscapes - 2 [20 points]\n",
        "\n",
        "Use the formula described in equation (1) in the paper in combination with the Filter-Wise Normalisation to generate landscape plots in these two cases:\n",
        "- your final trained model (output of question *6*).\n",
        "- your randomly initialised model.\n",
        "\n",
        "In both cases, use 25 values for $\\alpha$ and 25 values for $\\beta$ to generate a 2D plot with 625 points.\n",
        "\n",
        "Use the provided snippets of code in the Jupyter Notebook template to assist you in generating the plots and to guide you in the functions you will need to implement.\n",
        "\n",
        "Analyse, compare and discuss your plots. How are they different? and why? Justify the choices you make along the generation process of plotting the loss landscapes (for example, but not limited to, the range your choose for your $\\alpha$ and $\\beta$ values).\n",
        "\n",
        "[**you can use a subset of the data to generate the plots, but explain what effect this will have in the results**]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GxoB2bavIeUe"
      },
      "outputs": [],
      "source": [
        "# The following snippets of code are only to assist you. You can decide to use \n",
        "# them or not. They are only intended to provide you with some functionality\n",
        "#  you may find useful when trying to generate the loss landscape plots.\n",
        "\n",
        "\n",
        "# function to create random directions:\n",
        "def create_random_directions(weights, ignore1D=False, seed=42):\n",
        "    torch.manual_seed(seed)\n",
        "    direction = [torch.randn(w.size()).to(device) for w in weights]\n",
        "    \n",
        "    # apply filter normalisation, where every perturbation d in direction has the same norm as its corresponding w in weights\n",
        "    for d, w in zip(direction, weights):\n",
        "        if ignore1D and d.dim() <= 1:\n",
        "            d.fill_(0)\n",
        "        d.mul_(w.norm()/(d.norm() + 1e-10)) # add small perturbation to avoid division by zero\n",
        "\n",
        "    return direction\n",
        "\n",
        "\n",
        "# function to update weigths\n",
        "def update_weights(model, origin_weights, x_dir, y_dir, dx=0.1, dy=0.1):\n",
        "    updates = [x.to(device)*dx + y.to(device)*dy for (x, y) in zip(x_dir, y_dir)]\n",
        "    for (p, w, u) in zip(model.parameters(), origin_weights, updates):\n",
        "        p.data = w + u\n",
        "    return None\n",
        "\n",
        "\n",
        "# function to plot loss landscape as a surface\n",
        "def plot_loss_landscape(xx, yy, loss_landscape):\n",
        "    fig, ax = plt.subplots(figsize=(8, 8),subplot_kw={\"projection\": \"3d\"})\n",
        "    surf = ax.plot_surface(xx, yy, loss_landscape, cmap='viridis', edgecolor='none',\n",
        "                       linewidth=0, antialiased=True,  rstride=1, cstride=1,)\n",
        "    ax.set_xlabel(r'X')\n",
        "    ax.set_ylabel(r'Y')\n",
        "    ax.set_zlabel(r'Loss')\n",
        "    fig.colorbar(surf, shrink=0.5, aspect=5)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# function to plot loss landscape as a contour\n",
        "def contour_loss_landscape(xx, yy, loss_landscape):\n",
        "    fig, ax = plt.subplots(figsize=(7, 7))\n",
        "    surf = ax.contourf(xx, yy, loss_landscape, cmap='viridis', levels=100)\n",
        "    ax.set_xlabel(r'X')\n",
        "    ax.set_ylabel(r'Y')\n",
        "    fig.colorbar(surf, shrink=0.5, aspect=5)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# BONUS: functions to compute the angle between 2 random vectors\n",
        "# in high-dimensional spaces two random vectors are quite likely to be\n",
        "# orthogonal (or almost). No points involved here, this is just for fun!\n",
        "#\n",
        "def angle(vec1, vec2):\n",
        "    return torch.acos(torch.dot(vec1, vec2)/(vec1.norm()*vec2.norm())).item()\n",
        "\n",
        "def rad2deg(angle):\n",
        "    return angle*180/np.pi\n",
        "\n",
        "def concat_torch_list(torch_list):\n",
        "    for i, t in enumerate(torch_list):\n",
        "        torch_list[i] = t.flatten()\n",
        "    return torch.cat(torch_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WxVz-FhvKPUI"
      },
      "outputs": [],
      "source": [
        "# your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Cj5hz6lI4Kl"
      },
      "source": [
        "#### your answers here"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "ML_module_coursework_2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}